{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2373d7",
   "metadata": {},
   "source": [
    "# 3. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b9472",
   "metadata": {},
   "source": [
    "Data de entrega: 21/09/2025\n",
    "\n",
    "O enunciado da atividade está disponível neste [link](https://insper.github.io/ann-dl/versions/2025.2/exercises/mlp/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e349ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a17b11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercício 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a764e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, -0.2])\n",
    "y = 1\n",
    "\n",
    "W_hidden = np.array([[0.3, -0.1], [0.2, 0.4]])\n",
    "b_hidden = np.array([0.1, -0.2])\n",
    "\n",
    "W_output = np.array([0.5, -0.3])\n",
    "b_output = 0.2\n",
    "\n",
    "eta = 0.3\n",
    "\n",
    "# Activation function: tanh(x)\n",
    "activation_function = lambda x: (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n",
    "activation_function_derivative = lambda x: 1 - (activation_function(x)**2)\n",
    "\n",
    "# Loss function: MSE\n",
    "loss_function = lambda y, y_pred: 0.5 * (y - y_pred)**2\n",
    "loss_function_derivative = lambda y, y_pred: y - y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92420657",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4396f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "z1_pre = W_hidden @ x.T + b_hidden\n",
    "z1_activation = activation_function(z1_pre)\n",
    "\n",
    "# Output layer\n",
    "z2_pre = W_output @ z1_activation + b_output\n",
    "z2_activation = activation_function(z2_pre)\n",
    "\n",
    "y_pred = z2_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a6b65",
   "metadata": {},
   "source": [
    "### Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab2dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = loss_function(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f4b90",
   "metadata": {},
   "source": [
    "### Backward pass (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb771ba2",
   "metadata": {},
   "source": [
    "#### Erro na camada de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11915666",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error = loss_function_derivative(y, y_pred) * activation_function_derivative(z2_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f960a5e",
   "metadata": {},
   "source": [
    "#### Gradientes para os pesos e *bias* da saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1a6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_output_gradient = output_error * z1_activation\n",
    "b_output_gradient = output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1ecb0",
   "metadata": {},
   "source": [
    "#### Erro para a camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0e58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_error = output_error * W_hidden * activation_function_derivative(z1_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8b5a1",
   "metadata": {},
   "source": [
    "#### Gradientes para os pesos e *bias* das ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade19ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hidden_gradient = hidden_error * x\n",
    "b_hidden_gradient = hidden_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af816412",
   "metadata": {},
   "source": [
    "#### Atualização dos pesos e *biases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76a4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_output = W_output - eta * W_output_gradient\n",
    "b_output = b_output - eta * b_output_gradient\n",
    "\n",
    "W_hidden = W_hidden - eta * W_hidden_gradient\n",
    "b_hidden = b_hidden - eta * b_hidden_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3425b0",
   "metadata": {},
   "source": [
    "### Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9469942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_data = data.MLP(input=x,\n",
    "                        output=y,\n",
    "                        W_hidden=W_hidden,\n",
    "                        b_hidden=b_hidden,\n",
    "                        W_output=W_output,\n",
    "                        b_output=b_output,\n",
    "                        eta=eta,\n",
    "                        activation_function=activation_function,\n",
    "                        activation_function_d=activation_function_derivative,\n",
    "                        loss_function=loss_function,\n",
    "                        loss_function_d=loss_function_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ae8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_pre, z1_activation, z2_pre, z2_activation = mlp_data.forward()\n",
    "\n",
    "loss = mlp_data.loss_calculation(y, z2_activation)\n",
    "\n",
    "W_hidden_gradient, b_hidden_gradient, W_output_gradient, b_output_gradient = mlp_data.backpropagation(z1_pre, z1_activation, z2_pre, z2_activation)\n",
    "\n",
    "W_hidden, b_hidden, W_output, b_output = mlp_data.update_weights(W_hidden_gradient, b_hidden_gradient, W_output_gradient, b_output_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab65f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights hidden layer:\n",
      "[[ 0.22359454 -0.06819676]\n",
      " [ 0.12359454  0.43180324]]\n",
      "New bias hidden layer:\n",
      "[-0.05281093 -0.35901618]\n",
      "New weights output layer:\n",
      "[ 0.45670643 -0.27075481]\n",
      "New biases output layer:\n",
      "0.03577581279296005\n"
     ]
    }
   ],
   "source": [
    "print(f\"New weights hidden layer:\")\n",
    "print(f\"{W_hidden}\")\n",
    "print(f\"New bias hidden layer:\")\n",
    "print(f\"{b_hidden}\")\n",
    "print(f\"New weights output layer:\")\n",
    "print(f\"{W_output}\")\n",
    "print(f\"New biases output layer:\")\n",
    "print(f\"{b_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
