!!! abstract "Informa√ß√µes da entrega"
    üìÜ Deadline: 19/11/2025

    üìñ O enunciado da atividade est√° dispon√≠vel neste [link](https://insper.github.io/ann-dl/versions/2025.2/projects/generative).

As pipelines de modelos utilizadas nesse projeto foram feitas com base em modelos previamente treinados utilizando a ferramenta visual ComfyUI[^1]

## Text-to-image

A pipeline utilizada nesse exemplo √© encontrada na documenta√ß√£o[^2].

Para exemplificar o funcionamento desse workflow, utilizaremos como base o diagrama da [Figura 1](#figure-1).

<a id="figure-1" style="text-decoration: none; color: inherit; justify-content:center;">

| ![text-to-image-workflow](./img/text_to_image.drawio.svg) |
| :--: |
| **Figura 1**: Fluxograma do processo de gera√ß√£o de imagem a partir de um texto. **Fonte**: Autor. |

</a>

Como apontado anteriormente, o modelo foi pr√©-treinado e seus pesos carregados em um arquivo no formato `safetensors`, que √© um tipo de arquivo utilizado como alternativa aos de formato `pickle`, visto que apresenta os valores num√©ricos em forma de c√≥digo execut√°vel[^4]. Os par√¢metros do modelo s√£o carregados devidamente em cada um dos componentes da pipeline: no CLIP, no KSampler e no VAE.

Como podemos observar, o processo inicia com o condicionamento do modelo com *Contrastive Language‚ÄìImage Pre-training* (CLIP)[^3], seja ele positivo, ou seja, inserindo caracter√≠sticas que **s√£o** desejadas na imagem a ser gerada, ou negativo, inserindo caracter√≠sticas **n√£o** desejadas na imagem de sa√≠da.

Juntamente √† entrada, que √© uma imagem com ru√≠do, os condicionamentos servem de entrada para o KSampler, que aplica o [modelo treinado](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/resolve/main/v1-5-pruned-emaonly-fp16.safetensors?download=true) para remover o ru√≠do da imagem no espa√ßo latente. Dessa forma, a imagem sem ru√≠dos √© encaminhada para o decoder do *Variational Auto Encoder* (VAE), que decodifica a imagem no espa√ßo latente para uma imagem no formato original. 

O funcionamento da ferramenta pode ser visualizado pelo [V√≠deo 1](#video-1).

<a id="video-1" style="text-decoration: none; color: inherit; justify-content:center;">

| <iframe width="560" height="315" src="https://www.youtube.com/embed/8g8q5ul2rbw?si=AZb5PfP9nk11oGCn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> |
| :--: |
| **V√≠deo 1**: Funcionamento da ferramenta de acordo com as especifica√ß√µes. **Fonte**: Autor. |

</a>

[^1]:
    [ComfyUI | Generate video, images, 3D, audio with AI](https://www.comfy.org)

[^2]:
    [ComfyUI Text to Image Workflow](https://docs.comfy.org/tutorials/basic/text-to-image)

[^3]:
    [CLIP: Connecting text and images](https://openai.com/index/clip)

[^4]:
    [Safetensors](https://huggingface.co/docs/safetensors/index)