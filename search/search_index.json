{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ann-e-deep-learning","title":"ANN e Deep Learning","text":"Turma <p>2025.2</p>"},{"location":"#entregas","title":"Entregas","text":"Exerc\u00edcios <ul> <li> Data - Prazo 05/09/2025</li> <li> Perceptron - Prazo 14/09/2025</li> <li> MLP - Prazo 21/09/2025</li> <li> Metrics - Prazo --/--/2025</li> </ul> Projetos <ul> <li> Classifica\u00e7\u00e3o - Prazo --/--/2025</li> <li> Regress\u00e3o - Prazo --/--/2025</li> <li> Generative Models - Prazo --/--/2025</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"ex1_data/main/","title":"Data","text":""},{"location":"ex1_data/main/#exercicio-1","title":"Exerc\u00edcio 1","text":"<p>Os dados foram gerados por um script em Python, apresentado a seguir:</p> main.pyutils.py <pre><code>import matplotlib.pyplot as plt\n\nN = 400\n\ndef main():\n    class_0 = Data(mu=(2,  3), std=(.8,  2.5), n=N)\n    class_1 = Data(mu=(5,  6), std=(1.2, 1.9), n=N)\n    class_2 = Data(mu=(8,  1), std=(.9,   .9), n=N)\n    class_3 = Data(mu=(15, 4), std=(.5,  2.0), n=N)\n\n    x0, y0 = class_0.sample_initialize()\n    x1, y1 = class_1.sample_initialize()\n    x2, y2 = class_2.sample_initialize()\n    x3, y3 = class_3.sample_initialize()\n\n    plt.plot(x0, y0, \"o\", label=\"Classe 0\")\n    plt.plot(x1, y1, \"o\", label=\"Classe 1\")\n    plt.plot(x2, y2, \"o\", label=\"Classe 2\")\n    plt.plot(x3, y3, \"o\", label=\"Classe 3\")\n\n    plt.legend()\n\n    plt.title(\"Plot das classes\")\n\n    plt.show()\n\n    return 0\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>import numpy as np\n\n\nclass Data:\n    def __init__(self, mu, std, n):\n        self.mu_x, self.mu_y = mu\n        self.std_x, self.std_y = std\n        self.n = n\n\n    def sample_initialize(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        return np.random.normal(self.mu_x, self.std_x, self.n), np.random.normal(self.mu_y, self.std_y, self.n)\n\nclass MultiDimensionData:\n    def __init__(self, mu, cov, n):\n        self.mu = mu\n        self.cov = cov\n        self.n = n\n\n    def sample_initialize(self):\n        return np.random.multivariate_normal(self.mu, self.cov, self.n)\n</code></pre> <p>A imagem a seguir mostra o plot dos pontos gerados em para cada uma das classes, diferenciadas pela cor.</p> <p></p> <p>Podemos observar que, principalmente as classes 0 e 1 possuem um grande overlap, que tamb\u00e9m \u00e9 presente entre as classes 1 e 2, de maneira menos gritante. A classe 3 est\u00e1 completamente separada das outras tr\u00eas, quando observada visualmente.</p> <p>Dessa forma, podemos concluir que as classes poderiam ser separadas com linhas, mas que provavelmente existiriam alguns conflitos quanto \u00e0 classifica\u00e7\u00e3o das classes 0 e 1 e das classes 1 e 2.</p> <p>Abaixo segue uma representa\u00e7\u00e3o visual de como as linhas poderiam separar as classes.</p> <p></p>"},{"location":"ex1_data/main/#exercicio-2","title":"Exerc\u00edcio 2","text":"<p>As amostras foram geradas pelo c\u00f3digo apresentado abaixo:</p> main.pyutils.py <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef main():\n    mu_A = np.array([0, 0, 0, 0, 0])\n    cov_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n                    [0.8, 1.0, 0.3, 0.0, 0.0],\n                    [0.1, 0.3, 1.0, 0.5, 0.0],\n                    [0.0, 0.0, 0.5, 1.0, 0.2],\n                    [0.0, 0.0, 0.0, 0.2, 1.0]])\n\n    mu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n    cov_B = np.array([[ 1.5, -0.7, 0.2, 0.0, 0.0],\n                    [-0.7,  1.5, 0.4, 0.0, 0.0],\n                    [ 0.2,  0.4, 1.5, 0.6, 0.0],\n                    [ 0.0,  0.0, 0.6, 1.5, 0.3],\n                    [ 0.0,  0.0, 0.0, 0.3, 1.5]])\n\n    class_A = MultiDimensionData(mu=mu_A, cov=cov_A, n=500)\n    class_B = MultiDimensionData(mu=mu_B, cov=cov_B, n=500)\n\n    sample_A = class_A.sample_initialize()\n    sample_B = class_B.sample_initialize()\n\n    dataset = np.concatenate((sample_A, sample_B))\n\n    return 0\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>import numpy as np\n\n\nclass Data:\n    def __init__(self, mu, std, n):\n        self.mu_x, self.mu_y = mu\n        self.std_x, self.std_y = std\n        self.n = n\n\n    def sample_initialize(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        return np.random.normal(self.mu_x, self.std_x, self.n), np.random.normal(self.mu_y, self.std_y, self.n)\n\nclass MultiDimensionData:\n    def __init__(self, mu, cov, n):\n        self.mu = mu\n        self.cov = cov\n        self.n = n\n\n    def sample_initialize(self):\n        return np.random.multivariate_normal(self.mu, self.cov, self.n)\n</code></pre> <p>Em seguida, aplicou-se o conceito de PCA (Principal Component Analysis) para reduzir a dimensionalidade dos dados para duas dimens\u00f5es (2D).</p>"},{"location":"ex1_data/main/#passo-a-passo","title":"Passo-a-passo","text":"<p>Ap\u00f3s gerar as amostras (classes A e B), \u00e9 necess\u00e1rio obter a matriz de covari\u00e2ncia dos dados como um todo.</p> <pre><code>mat = np.cov(dataset, rowvar=False)\n</code></pre> <p>Depois disso, precisamos obter os autovalores e os autovetores dessa matriz, sendo que os autovalores servir\u00e3o para auxiliar na defini\u00e7\u00e3o da import\u00e2ncia das features e os autovetores s\u00e3o essenciais para que possamos obter um novo conjunto de amostras, agora apenas com as features selecionadas.</p> <pre><code># Obten\u00e7\u00e3o dos autovalores e autovetores\neigenvalues, eigenvectors = np.linalg.eig(mat)\n\n# Processo feito para ordenar a lista de autovetores e autovalores\n## Obt\u00e9m os \u00edndices que ordenariam o vetor e inverte a lista\nidx = np.argsort(eigenvalues)[::-1]\n\n## Ordena a lista de autovalores\neigenvalues = eigenvalues[idx]\n\n## Ordena a lista de autovetores (colunas)\neigenvectors = eigenvectors[:, idx]\n\n# Obt\u00e9m os dois principais autovetores (para PC1 e PC2)\npcs = eigenvectors[:, :2] # matrix 5x2\n\n# Centralizar o dataset original \ndataset_mu = dataset.mean(axis=0) # matriz 1000x5\ndataset_cent = dataset - dataset_mu\n\n# Obten\u00e7\u00e3o do novo conjunto de dados\nZ = dataset_cent @ pcs # (1000,5) x (5, 2)\n</code></pre> <p>Nota-se que foram realizadas algumas outras etapas antes de obtermos o novo conjunto de amostras, que foram realizadas para que esse conjunto estivesse centralizado.</p> <p>Por fim, podemos plotar o gr\u00e1fico com as duas features selecionadas e separ\u00e1-las de acordo com as respectivas classes.</p> <p></p> <p>De acordo com a imagem, observa-se que os dados da classe B tendem mais a valores negativos, enquanto os da classes A tendem mais a valores positivos.</p> <p>O problema surge pois existe uma grande quantidade de dados que s\u00e3o semelhantes, tornando o uso de modelos simples para classifica\u00e7\u00e3o linear inadequados para classificar as classes. Seria necess\u00e1rio o uso de ferramentas mais robustas como um MLP, que possibilitam uma propaga\u00e7\u00e3o de erro em camadas para que o modelo seja treinado de forma mais eficiente (backpropagation).</p>"},{"location":"ex1_data/main/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"ex1_data/main/#objetivo-do-dataset","title":"Objetivo do dataset","text":"<p>O dataset apresenta como objetivo prever se um passageiro foi transportado para uma outra dimens\u00e3o durante uma colis\u00e3o da nave espacial Titanic com uma anomalia espa\u00e7o-temporal. Para isso, s\u00e3o disponibilizados dados que foram recuperados dos registros pessoais dos passageiros do sistema da nave.</p>"},{"location":"ex1_data/main/#descricao-das-features","title":"Descri\u00e7\u00e3o das features","text":"<p>Existem 14 features diferentes do dataset a ser analisado. Podemos separ\u00e1-las em num\u00e9ricas e em categ\u00f3ricas, como mostrado a seguir:</p> <ul> <li> <p>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>;</p> </li> <li> <p>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>, <code>Name</code>, <code>Transported</code>.</p> </li> </ul>"},{"location":"ex1_data/main/#valores-ausentes","title":"Valores ausentes","text":"<p>Podemos observar na imagem abaixo a quantidade de valores nulos por feature.</p> <p></p>"},{"location":"ex1_data/main/#pre-processamento-dos-dados","title":"Pr\u00e9-processamento dos dados","text":"<p>Para cada tipo de feature, os dados faltantes foram tratados de maneiras diferentes:</p> <ul> <li> <p>categ\u00f3ricas (bin\u00e1rias e nominais): foi extra\u00edda a moda da coluna e os valores ausentes foram preenchidos por ela, visto que \u00e9 uma estrat\u00e9gia simples, mas que contorna o problema de impossibilitar o one-hot encoding, por exemplo.</p> </li> <li> <p>num\u00e9ricas: foi extra\u00edda a mediana e os valores ausentes preenchidos por ela, da mesma forma, \u00e9 uma t\u00e9cnica simples que n\u00e3o exige muito tratamento, al\u00e9m de garantir roubstez a outliers, algo que o uso da m\u00e9dia n\u00e3o possibilitaria.</p> </li> </ul> <p>Dessa forma, apesar de o dataset sofrer um leve desbalanceamento, os dados puderam ser mantidos em vez de remover linhas inteiras que contivessem valores nulos, mantendo a integridade da base de dados.</p> <p>Al\u00e9m disso, a feature <code>Cabin</code> foi subdividida em 3 categorias menores: <code>CabinDeck</code>, <code>CabinNum</code> e <code>CabinSide</code>, como \u00e9 descrito no site do Kaggle.</p>"},{"location":"ex1_data/main/#fazendo-one-hot-encoding-de-features-categoricas","title":"Fazendo one-hot encoding de features categ\u00f3ricas","text":"<p>Para features como <code>HomePlanet</code>, <code>Destination</code>, <code>CabinDeck</code> e <code>CabinSide</code> (derivadas da feature <code>Cabin</code>), foi feito one-hot encoding para transform\u00e1-las em vari\u00e1veis categ\u00f3ricas de ordem bin\u00e1ria, sendo uma das etapas para possibilitar a implementa\u00e7\u00e3o de uma rede neural cuja fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a tangente hiperb\u00f3lica (\\(tanh(x)\\)).</p>"},{"location":"ex1_data/main/#padronizacao-dos-dados-z-score","title":"Padroniza\u00e7\u00e3o dos dados (z-score)","text":"<p>Em seguida, os dados foram tratados de forma que as features num\u00e9ricas possu\u00edssem m\u00e9dia \\(0\\) (\\(\\mu = 0\\)) e desvio padr\u00e3o \\(1\\) (\\(\\sigma = 0\\)). Essa \u00e9 outra etapa para que seja poss\u00edvel realizar o treinamento da rede neural utilizando a fun\u00e7\u00e3o tanh(x) como fun\u00e7\u00e3o de ativa\u00e7\u00e3o, visto que o dom\u00ednio da fun\u00e7\u00e3o est\u00e1 definido no intervalo \\([-1, 1]\\).</p>"},{"location":"ex1_data/main/#visualizacao-dos-resultados","title":"Visualiza\u00e7\u00e3o dos resultados","text":"<p>O primeiro histograma mostra a compara\u00e7\u00e3o de como era a distribui\u00e7\u00e3o das idades ANTES do tratamento dos dados e como ficou AP\u00d3S o tratamento.</p> <p></p> <p>Podemos observar que a quantidade de pessoas \u00e0 bordo na faixa de 20 anos se mostra maior quando os dados n\u00e3o foram tratados. Ap\u00f3s o tratamento, a faixa muda para 25 anos.</p> <p>Em seguida, temos a compara\u00e7\u00e3o para duas vari\u00e1veis semelhantes, que abordam os gastos na pra\u00e7a de alimenta\u00e7\u00e3o da nave e com servi\u00e7os de quarto, respectivamente.</p> <p></p> <p></p> <p>Conseguimos, a partir dos gr\u00e1ficos, concluir que no caso dessas duas vari\u00e1veis, n\u00e3o houverem mudan\u00e7as significativas, uma vez que boa parte dos passageiros n\u00e3o gastou com esses dois servi\u00e7os.</p>"},{"location":"ex2_perceptron/main/","title":"Perceptron","text":"<p>O enunciado da atividade est\u00e1 dispon\u00edvel neste link.</p> <p>Para trabalhar nos exerc\u00edcios, foi feita um arquivo de utilidades (<code>./utils/data.py</code>) com algumas fun\u00e7\u00f5es comuns que foram utilizadas em outras situa\u00e7\u00f5es tamb\u00e9m.</p> <pre><code>import numpy as np\n\n\nclass Data:\n    def __init__(self, mu, std, n):\n        self.mu_x, self.mu_y = mu\n        self.std_x, self.std_y = std\n        self.n = n\n\n    def sample_initialize(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        return np.random.normal(self.mu_x, self.std_x, self.n), np.random.normal(self.mu_y, self.std_y, self.n)\n\nclass MultiDimensionData:\n    def __init__(self, mu: list, cov: list, n: int):\n        self.mu = np.array(mu)\n        self.cov = np.array(cov)\n        self.n = n\n\n    def sample_initialize(self):\n        return np.random.multivariate_normal(self.mu, self.cov, self.n)\n</code></pre> <p>E importamos isso no arquivo de execu\u00e7\u00e3o do c\u00f3digo.</p> <pre><code>from utils import data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n</code></pre>"},{"location":"ex2_perceptron/main/#geracao-de-dados","title":"Gera\u00e7\u00e3o de dados","text":"<p>Foram geradas 2000 amostras de dados, utilizando uma distribui\u00e7\u00e3o normal (Gaussiana) multivariada, sendo que metade dos dados fazem parte de uma classe e a outra metade de outra (denominadas classes 0 e 1)</p> <pre><code>class_0 = data.MultiDimensionData(mu=[mu1, mu2],\n                                  cov=[[cov11, cov12], [cov21, cov22]],\n                                  n=N)\n\nclass_1 = data.MultiDimensionData(mu=[mu1, mu2],\n                                  cov=[[cov11, cov12], [cov21, cov22]],\n                                  n=N)\n\nfeatures = np.concatenate((class_0.sample_initialize(), class_1.sample_initialize()))\n\nlabels = np.concatenate((np.zeros(N, dtype=int), np.ones(N, dtype=int)))\n\nshuffled_features, shuffled_labels = shuffle_sample(sample_array=features, labels_array=labels)\n</code></pre> <p>sendo <code>N = 1000</code>.</p> <p>Foram utilizados diferentes valores para m\u00e9dia e covari\u00e2ncia para cada um dos exerc\u00edcios, disponibilizados no link com o enunciado do exerc\u00edcio.</p> <p>Tamb\u00e9m precisamos levar em conta que os dados foram embaralhados para evitar enviesamento durante o treinamento do modelo.</p> <p>Para fins pr\u00e1ticos, iremos referenci\u00e1-los aqui como linearmente separ\u00e1veis (i - exerc\u00edcio 1) e sobrepostos (ii - exerc\u00edcio 2).</p> <p>O plot dos gr\u00e1ficos para (i) e (ii) se encontra abaixo, respectivamente.</p> Figura 1 - Amostragem com dados linearmente separados Figura 2 - Amostragem com dados sobrepostos"},{"location":"ex2_perceptron/main/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do perceptron","text":"<p>Foram definidas algumas fun\u00e7\u00f5es que foram utilizadas para o perceptron.</p> Shuffle <pre><code>def shuffle_sample(sample_array, labels_array):\n    lista = list(zip(sample_array, labels_array))\n    random.shuffle(lista)\n\n    features, labels = zip(*lista)\n    return np.array(features), np.array(labels)\n</code></pre> M\u00e9tricas <pre><code>def confusion_matrix(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    VP = np.sum((y_true == 1) &amp; (y_pred == 1))  # verdadeiros positivos\n    VN = np.sum((y_true == 0) &amp; (y_pred == 0))  # verdadeiros negativos\n    FP = np.sum((y_true == 0) &amp; (y_pred == 1))  # falsos positivos\n    FN = np.sum((y_true == 1) &amp; (y_pred == 0))  # falsos negativos\n\n    return np.array([[VN, FP],\n                     [FN, VP]])\n\ndef accuracy(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    return np.mean(y_true == y_pred)\n</code></pre> Processamento <pre><code>def forward(x, y, activation, n_epochs, eta=.01):\n    W = np.array([[0, 0]])\n    b = 0\n\n    acc_array = []\n    total_epochs = 0\n\n    for i in range(n_epochs):\n        print(f\"[epoch {i+1}] Starting...\")\n\n        total_epochs = i+1\n        y_pred_vec = []\n        updated = 0\n\n        for j in range(x.shape[0]):\n            z = np.dot(W, x[j].T) + b\n\n            y_pred = activation(z)\n\n            y_pred_vec.append(y_pred)\n\n            error = y[j] - y_pred\n\n            if error != 0:\n                updated += 1\n                W = W + eta * error * x[j]\n                b = b + eta * error\n\n        acc = accuracy(y, y_pred_vec)\n        acc_array.append(acc)\n\n        print(f\"- Accuracy: {acc}\")\n\n        if not updated:\n            print(f\"- No updates detected...\")\n\n            break\n\n    print(f\"Training finished after {total_epochs} epochs.\")\n    return W, b, y_pred_vec\n</code></pre> <p>No exerc\u00edcio 1, como esperado, o modelo converge ap\u00f3s algumas \u00e9pocas (menos que 100), visto que os dados s\u00e3o linearmente separ\u00e1veis.</p> <p>O gr\u00e1fico abaixo mostra a linha de separa\u00e7\u00e3o entre as classes, calculada com base nos pesos e no bias obtidos pelo forward pass ap\u00f3s o treinamento.</p> Figura 3 - Linha de separa\u00e7\u00e3o para a amostra (i) <p>Em seguida, calculamos para os dados sobrepostos, em que o esperado era que o modelo n\u00e3o fosse capaz de convergir, visto que precisamos de um modelo mais complexo (por exemplo, MLP ou SVM) para separar os dados de forma mais correta. Ao rodar o c\u00f3digo, foi obtida uma acur\u00e1cia de aproximadamente \\(60\\%\\), e a linha de decis\u00e3o pode ser vista na imagem abaixo.</p> Figura 4 - Linha de separa\u00e7\u00e3o para a amostra (ii)"},{"location":"ex3_mlp/main/","title":"MLP","text":"<p>Informa\u00e7\u00f5es da entrega</p> <p>Data 21/09/2025 O enunciado da atividade est\u00e1 dispon\u00edvel neste link.</p> <p>Para essa atividade, uma sequ\u00eancia de passos foi seguida a fim de garantir a execu\u00e7\u00e3o correta do MLP:</p> <ol> <li> <p>Inicializa\u00e7\u00e3o da amostra;</p> <ul> <li>Amostragem (\\(x\\)): \\(n_{features} \\times n_{amostras}\\)</li> <li>R\u00f3tulos (\\(y\\)): \\(n_{outputs} \\times n_{amostras}\\)</li> </ul> </li> <li> <p>Defini\u00e7\u00e3o de hiperpar\u00e2metros para o treinamento do modelo;</p> <ul> <li>Pesos da camada oculta: (\\(W^{(1)}\\)) \\(n_{features} \\times n_{neur\u00f4nios}\\)</li> <li>Bias da camada oculta(\\(b^{(1)}\\)): \\(n_{neur\u00f4nios} \\times n_{amostras}\\)</li> <li>Pesos da camada de sa\u00edda (\\(W^{(2)}\\)): \\(n_{neur\u00f4nios} \\times n_{sa\u00eddas}\\)</li> <li>Bias da camada de sa\u00edda (\\(b^{(2)}\\)): \\(n_{sa\u00eddas} \\times n_{amostras}\\)</li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: \\(f(x)\\) </li> <li>Derivada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o: \\(f'(x)\\)</li> <li>Fun\u00e7\u00e3o de perda: \\(\\mathcal{L}\\)</li> </ul> </li> <li> <p>Treino;</p> </li> <li> <p>Teste.</p> </li> </ol>"},{"location":"ex3_mlp/main/#exercicio-1-calculo-manual-das-etapas-para-um-multi-layer-perceptron-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo manual das etapas para um Multi-Layer Perceptron (MLP)","text":"<ul> <li>Passo 1</li> </ul> Inicializa\u00e7\u00e3o da amostra main.py<pre><code>samples_1 = np.array([[0.5, -0.2]]).T   # n_features X n_samples \nlabels_1 = np.array([[1]])              # n_outputs X n_samples\n</code></pre> <ul> <li>Passo 2</li> </ul> Defini\u00e7\u00e3o de hiperpar\u00e2metros main.py<pre><code>W1_1 = np.array([[0.3, 0.2], [-0.1, 0.4]])  # n_features X n_neurons\nb1_1 = np.array([[0.1], [-0.2]])            # n_neurons X n_samples\n\nW2_1 = np.array([[0.5], [-0.3]])            # n_neurons X n_outputs\nb2_1 = np.array([[0.2]])                    # n_outputs X n_samples\n\nactivation_function_1 = lambda x: (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\nactivation_function_1_derivative = lambda x: 1 - (activation_function_1(x)**2)\n\nloss_function_1 = lambda y, y_pred: 0.5 * (y - y_pred)**2\nloss_function_1_derivative = lambda y, y_pred: y - y_pred\n</code></pre> <p>Foi utilizada, conforme o enunciado do exerc\u00edcio, a fun\u00e7\u00e3o de perda Mean Squared Error (MSE) e a fun\u00e7\u00e3o de ativa\u00e7\u00e3o \\(tanh(x)\\).</p> <ul> <li>Passo 3</li> </ul> TreinamentoClasse main.py<pre><code>kwargs_1 = {\n            \"input\": samples_1, \n            \"output\": labels_1, \n            \"W_hidden\": W1_1, \n            \"b_hidden\": b1_1, \n            \"W_output\": W2_1, \n            \"b_output\": b2_1, \n            \"eta\": eta_1, \n            \"hidden_activation\": activation_function_1, \n            \"hidden_activation_d\": activation_function_1_derivative, \n            \"output_activation\": activation_function_1, \n            \"output_activation_d\": activation_function_1_derivative, \n            \"loss_function\": loss_function_1, \n            \"loss_function_d\": loss_function_1_derivative\n        }\n\nmlp_data_train_1 = data.MLP(**kwargs_1)\n\nz1, h1, z2, y_pred = mlp_data_train_1.forward()\n\nloss = mlp_data_train_1.loss_calculation(labels_1, y_pred)\n\ndW1_1, db1_1, dW2_1, db2_1 = mlp_data_train_1.backpropagation(z1, h1, z2, y_pred)\n\nW_hidden, b_hidden, W_output, b_output = mlp_data_train_1.update_weights(dW1_1, db1_1, dW2_1, db2_1)\n</code></pre> data.py<pre><code>class MLP:\n\n    def __init__(self, **kwargs):\n        self.input  = kwargs.get(\"input\")\n        self.output = kwargs.get(\"output\")\n        self.W_hidden = kwargs.get(\"W_hidden\")\n        self.b_hidden = kwargs.get(\"b_hidden\")\n        self.W_output = kwargs.get(\"W_output\")\n        self.b_output = kwargs.get(\"b_output\")\n        self.eta = kwargs.get(\"eta\", 0.001)\n\n        # Hidden layer\n        self.hidden_activation   = kwargs.get(\"hidden_activation\")\n        self.hidden_activation_d = kwargs.get(\"hidden_activation_d\")\n\n        # Output layer (opcional)\n        self.output_activation   = kwargs.get(\"output_activation\", None)\n        self.output_activation_d = kwargs.get(\"output_activation_d\", None)\n\n        # Loss\n        self.loss_function   = kwargs.get(\"loss_function\")\n        self.loss_function_d = kwargs.get(\"loss_function_d\")\n\n    def forward(self):\n        # Hidden layer\n        # z1_pre: (n_neurons X n_samples); \n        # W1: (n_neurons X n_feat); input: (n_feat X n_samples); b1: (n_neurons X n_samples)\n        z1_pre = self.W_hidden.T @ self.input + self.b_hidden\n        z1_act = self.hidden_activation(z1_pre)\n\n        # Output layer\n        # z2_pre: (n_outputs X n_samples); \n        # W2: (n_outputs X n_neurons); z1_act: (n_neurons X n_samples); b2: (n_outputs X n_samples)\n        z2_pre = self.W_output.T @ z1_act + self.b_output\n\n        if self.output_activation:\n            z2_act = self.output_activation(z2_pre)\n        else:\n            z2_act = z2_pre\n\n        return z1_pre, z1_act, z2_pre, z2_act\n\n    def loss_calculation(self, true_label, predicted_label):\n        return self.loss_function(true_label, predicted_label)\n\n    def backpropagation(self, z1_pre, z1_act, z2_pre, z2_act):\n        # formato n_output X n_samples\n        output_error = self.loss_function_d(self.output, z2_act)\n\n        if self.output_activation_d:\n            output_error *= self.output_activation_d(z2_pre) \n\n        # formato n_neurons X n_samples\n        hidden_error = (self.W_output @ output_error) * self.hidden_activation_d(z1_pre)\n\n        # Gradientes\n        W_output_gradient = z1_act @ output_error.T\n        b_output_gradient = np.sum(output_error, axis=1, keepdims=True)\n        W_hidden_gradient = self.input @ hidden_error.T\n        b_hidden_gradient = np.sum(hidden_error, axis=1, keepdims=True)\n\n        return W_hidden_gradient, b_hidden_gradient, W_output_gradient, b_output_gradient\n\n    def update_weights(self, W_hidden_gradient, b_hidden_gradient,\n                    W_output_gradient, b_output_gradient):\n        self.W_hidden -= self.eta * W_hidden_gradient\n        self.b_hidden -= self.eta * b_hidden_gradient\n        self.W_output -= self.eta * W_output_gradient\n        self.b_output -= self.eta * b_output_gradient\n        return self.W_hidden, self.b_hidden, self.W_output, self.b_output\n</code></pre> <p>Os resultados do backward pass para as camadas oculta (1) e de sa\u00edda (2) foram:</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} \\approx \\begin{bmatrix} 0.26179727 &amp; 0.22385243 \\cr -0.08471891 &amp; 0.39045903 \\end{bmatrix}\\)</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} \\approx \\begin{bmatrix} 0.02359454  \\cr -0.15229515 \\end{bmatrix}\\)</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} \\approx \\begin{bmatrix} 0.45670643 \\cr -0.27075481 \\end{bmatrix}\\)</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} \\approx 0.03577581\\)</p>"},{"location":"ex3_mlp/main/#exercicio-2-classificacao-binaria-com-dados-sinteticos","title":"Exerc\u00edcio 2: Classifica\u00e7\u00e3o bin\u00e1ria com dados sint\u00e9ticos","text":"<ul> <li>Passo 1</li> </ul> Inicializa\u00e7\u00e3o da amostra {main.py<pre><code>N_FEATURES_2 = 2\nN_OUTPUT_2 = 1\nN_NEURONS_2 = 10\nSAMPLE_SIZE_2 = 1000\nTRAIN_SIZE = .8\n\nsamples_1_2, samples_labels_1_2 = make_classification(n_samples=SAMPLE_SIZE_2 // 2,\n                                                n_classes=1,\n                                                n_clusters_per_class=1,\n                                                n_features=N_FEATURES_2,\n                                                n_informative=2,\n                                                n_redundant=0,\n                                                random_state=21,\n                                                class_sep=2.0)\n\nsamples_2_2, samples_labels_2_2 = make_classification(n_samples=SAMPLE_SIZE_2 // 2,\n                                                n_classes=1,\n                                                n_clusters_per_class=2,\n                                                n_features=N_FEATURES_2,\n                                                n_informative=2,\n                                                n_redundant=0,\n                                                random_state=42,\n                                                class_sep=2.0)\n\nsamples_labels_1_2[:] = 0\nsamples_labels_2_2[:] = 1\n\nsamples_total_2 = np.concatenate((samples_1_2, samples_2_2))\nsamples_total_labels_2 = np.concatenate((samples_labels_1_2, samples_labels_2_2))\n\nshuffled_samples_total_2, shuffled_samples_total_labels_2 = data.shuffle_sample(sample_array=samples_total_2, labels_array=samples_total_labels_2)\n</code></pre> <p>O tamanho da amostragem \u00e9 de 1000, com 2 classes, 2 features e 16 neur\u00f4nios para a camada oculta. Como \u00e9 um problema de classifica\u00e7\u00e3o bin\u00e1ria, o n\u00famero de outputs \u00e9 de 1 (0 ou 1).</p> <p>A imagem (ref) ilustra graficamente a rela\u00e7\u00e3o entre as features.</p> Figura 1: Amostragem para o exerc\u00edcio 2 <ul> <li>Passo 2</li> </ul> Defini\u00e7\u00e3o de hiperpar\u00e2metros main.py<pre><code>val = (6 / (N_FEATURES_2 + N_OUTPUT_2))**.5\n\nW1_2 = np.random.uniform(-val, val, size=(N_FEATURES_2, N_NEURONS_2))\nW2_2 = np.random.uniform(-val, val,  size=(N_NEURONS_2, N_OUTPUT_2))\n\nb1_2 = np.zeros((N_NEURONS_2, 1)) # n_neurons X n_sample (train)\nb2_2 = np.zeros((N_OUTPUT_2, 1))\n\ntrain_sample_2, test_sample_2, train_sample_labels_2, test_sample_labels_2 = data.train_test_split(shuffled_samples_total_2, shuffled_samples_total_labels_2, TRAIN_SIZE)\n\ntrain_sample_2 = train_sample_2.T\ntest_sample_2 = test_sample_2.T\n\nmu  = np.mean(train_sample_2, axis=1, keepdims=True)\nstd = np.std(train_sample_2, axis=1, keepdims=True) + 1e-8\n\ntrain_sample_norm_2 = (train_sample_2 - mu) / std\ntest_sample_norm_2  = (test_sample_2  - mu) / std\n\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\nsigmoid_d = lambda x: sigmoid(x) * (1 - sigmoid(x))\n\neps = 1e-8\nbce = lambda y, y_pred: -(y * np.log(y_pred + eps) + (1 - y) * np.log(1 - y_pred + eps))\nbce_d = lambda y, y_pred: (y_pred - y) / ((y_pred + eps) * (1 - y_pred + eps))\n</code></pre> <p>Aqui, inicializamos os pesos com o m\u00e9todo de Xavier/Glorot. No exerc\u00edcio, \u00e9 utilizada a inicializa\u00e7\u00e3o uniforme de Xavier, definida pela equa\u00e7\u00e3o \\(2.1\\).</p> \\[ x = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\quad \\text{(2.1)} \\] <p>Foi utilizada a fun\u00e7\u00e3o de ativa\u00e7\u00e3o sigmoide, pois os valores est\u00e3o normalizados, juntamente \u00e0 fun\u00e7\u00e3o de perda Binary Cross-Entropy (BCE), por se tratar de uma classifica\u00e7\u00e3o bin\u00e1ria.</p> <ul> <li>Passo 3</li> </ul> Treinamento train.py<pre><code>kwargs_2 = {\"input\": train_sample_norm_2, \n        \"output\": train_sample_labels_2, \n        \"W_hidden\": W1_2, \n        \"b_hidden\": b1_2, \n        \"W_output\": W2_2, \n        \"b_output\": b2_2, \n        \"eta\": .001, \n        \"hidden_activation\": sigmoid, \n        \"hidden_activation_d\": sigmoid_d, \n        \"output_activation\": sigmoid, \n        \"output_activation_d\": sigmoid_d, \n        \"loss_function\": bce, \n        \"loss_function_d\": bce_d}\n\nmlp_object_train_2 = data.MLP(**kwargs_2)\n\nepoch_losses = {100: [], 300: [], 500: []}\nepoch_accuracy = {}\n\nfor n_epochs, losses in epoch_losses.items():\n    epoch_accuracy[n_epochs] = []\n\n    for epoch in range(n_epochs):\n        z1_pre, z1_activation, z2_pre, z2_activation = mlp_object_train_2.forward()\n\n        loss = mlp_object_train_2.loss_calculation(train_sample_labels_2, z2_activation)\n        losses.append(np.mean(loss))\n\n        y_pred = (z2_activation &gt; 0.5).astype(int)\n        acc = np.mean(y_pred == train_sample_labels_2)\n        epoch_accuracy[n_epochs].append(acc)\n\n        W_hidden_gradient, b_hidden_gradient, W_output_gradient, b_output_gradient = mlp_object_train_2.backpropagation(z1_pre, z1_activation, z2_pre, z2_activation)\n\n        W_hidden, b_hidden, W_output, b_output = mlp_object_train_2.update_weights(W_hidden_gradient, b_hidden_gradient, W_output_gradient, b_output_gradient)\n</code></pre> <p>O treinamento foi realizado utilizando 100, 300 e 500 \u00e9pocas, e foi avaliada de acordo com as perdas ao longo do treinamento, assim como a acur\u00e1cia obtida.</p> Figura 2: Perda ao longo do treinamento Figura 3: Acur\u00e1cia ao longo do treinamento <ul> <li>Passo 4</li> </ul> Teste main.py<pre><code>kwargs_test_2 = {\n                \"input\": test_sample_norm_2, \n                \"output\": test_sample_labels_2, \n                \"W_hidden\": W_hidden, \n                \"b_hidden\": b_hidden, \n                \"W_output\": W_output, \n                \"b_output\": b_output, \n                \"eta\": .001, \n                \"hidden_activation\": sigmoid, \n                \"hidden_activation_d\": sigmoid_d, \n                \"output_activation\": sigmoid, \n                \"output_activation_d\": sigmoid_d, \n                \"loss_function\": bce, \n                \"loss_function_d\": bce_d\n            }\n\nmlp_object_test_2 = data.MLP(**kwargs_test_2)\n\nz1_test_2, h1_test_2, z2_test_2, y_pred_test_2 = mlp_object_test_2.forward()\n\nloss_test_2 = mlp_object_test_2 = data.MLP(**kwargs_test_2).loss_calculation(test_sample_labels_2, y_pred_test_2)\n\nTHRESHOLD = .5\n\ny_pred = (y_pred_test_2 &gt; THRESHOLD).astype(int)\nacc_test = np.mean(y_pred == test_sample_labels_2)\n</code></pre> <p>Ap\u00f3s o treinamento, foi obtida uma acur\u00e1cia de \\(90.50\\%\\), como esperado de acordo com o gr\u00e1fico ilustrado na figura 3.</p>"},{"location":"ex3_mlp/main/#exercicio-3-classificacao-multi-classe-com-dados-sinteticos","title":"Exerc\u00edcio 3: Classifica\u00e7\u00e3o multi-classe com dados sint\u00e9ticos","text":"<ul> <li>Passo 1</li> </ul> Inicializa\u00e7\u00e3o da amostra main.py<pre><code>SAMPLE_SIZE_3           = 1500\nN_FEATURES_3            = 4\nN_INFORMATIVE_3         = 4\nN_REDUNDANT_3           = 0\nrandom_state            = {\"classe 0\": 21, \n                    \"classe 1\": 42, \n                    \"classe 2\": 84}\nn_cluters_per_class     = {\"classe 0\": 2, \n                    \"classe 1\": 3, \n                    \"classe 2\": 4}\nCLASS_SEP_3             = 2.0\nN_CLASSES_3             = 3\nN_NEURONS_3             = 128\n\nsamples_0_3, samples_labels_0_3 = make_classification(n_samples=SAMPLE_SIZE_3 // 3,\n                                                n_classes=1,\n                                                n_clusters_per_class=n_cluters_per_class[\"classe 0\"],\n                                                n_features=N_FEATURES_3,\n                                                n_informative=N_INFORMATIVE_3,\n                                                n_redundant=N_REDUNDANT_3,\n                                                random_state=random_state[\"classe 0\"],\n                                                class_sep=CLASS_SEP_3)\n\nsamples_1_3, samples_labels_1_3 = make_classification(n_samples=SAMPLE_SIZE_3 // 3,\n                                                n_classes=1,\n                                                n_clusters_per_class=n_cluters_per_class[\"classe 1\"],\n                                                n_features=N_FEATURES_3,\n                                                n_informative=N_INFORMATIVE_3,\n                                                n_redundant=N_REDUNDANT_3,\n                                                random_state=random_state[\"classe 1\"],\n                                                class_sep=CLASS_SEP_3)\n\nsamples_2_3, samples_labels_2_3 = make_classification(n_samples=SAMPLE_SIZE_3 // 3,\n                                                n_classes=1,\n                                                n_clusters_per_class=n_cluters_per_class[\"classe 2\"],\n                                                n_features=N_FEATURES_3,\n                                                n_informative=N_INFORMATIVE_3,\n                                                n_redundant=N_REDUNDANT_3,\n                                                random_state=random_state[\"classe 2\"],\n                                                class_sep=CLASS_SEP_3)\n\nsamples_labels_0_3[:] = 0\nsamples_labels_1_3[:] = 1\nsamples_labels_2_3[:] = 2\n\nsamples_total_3 = np.concatenate((samples_0_3, samples_1_3, samples_2_3))\nsamples_total_labels_3 = np.concatenate((samples_labels_0_3, samples_labels_1_3, samples_labels_2_3))\n\nshuffled_samples_total_3, shuffled_samples_total_labels_3 = data.shuffle_sample(sample_array=samples_total_3, labels_array=samples_total_labels_3)\n</code></pre> <p>A figura 4 mostra um gr\u00e1fico da distribui\u00e7\u00e3o das amostras em rela\u00e7\u00e3o \u00e0 2 features.</p> Figura 4: Amostragem para o exerc\u00edcio 3 <ul> <li>Passo 2</li> </ul> Defini\u00e7\u00e3o dos hiperpar\u00e2metros main.py<pre><code>val = (6 / (N_FEATURES_3 + N_CLASSES_3))**.5\n\nW1_3 = np.random.uniform(-val, val, size=(N_FEATURES_3, N_NEURONS_3))\nW2_3 = np.random.uniform(-val, val, size=(N_NEURONS_3, N_CLASSES_3))\n\nb1_3 = np.zeros((N_NEURONS_3, 1))\nb2_3 = np.zeros((N_CLASSES_3, 1))\n\ntrain_sample_3, test_sample_3, train_sample_labels_3, test_sample_labels_3 = data.train_test_split(shuffled_samples_total_3, shuffled_samples_total_labels_3)\n\ntrain_sample_3 = train_sample_3.T\ntest_sample_3 = test_sample_3.T\n\nmu  = np.mean(train_sample_3, axis=1, keepdims=True)\nstd = np.std(train_sample_3, axis=1, keepdims=True) + 1e-8\n\ntrain_sample_norm_3 = (train_sample_3 - mu) / std\ntest_sample_norm_3  = (test_sample_3  - mu) / std\n\ntanh = lambda x: (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\ntanh_d = lambda x: 1 - tanh(x)**2\n\ndef softmax(z):\n    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\ndef cce(y, y_pred, eps=1e-8):\n    N = y.shape[1]\n    return -np.sum(y * np.log(y_pred + eps)) / N\n\ndef cce_d(y, y_pred):\n    N = y.shape[1]\n    return (y_pred - y) / N\n</code></pre> <p>Nesse exerc\u00edcio, utilizamos a fun\u00e7\u00e3o de ativa\u00e7\u00e3o \\(tanh(x)\\), visto que a amostra foi normalizada, e \\(softmax(x)\\), que \u00e9 adequada para problemas de classifica\u00e7\u00e3o multi-classe. Para avalia\u00e7\u00e3o, foi utilizada a fun\u00e7\u00e3o de perda Categorical Cross-Entropy.</p> <ul> <li>Passo 3</li> </ul> Treinamento main.py<pre><code>THRESHOLD_3 = .5\nactivation_array = [tanh, softmax]\nactivation_d_array = [tanh_d, None]\n\nkwargs_train_3 = {\n                \"input\": train_sample_norm_3, \n                \"output\": train_labels_onehot_3, \n                \"W_hidden\": W1_3, \n                \"b_hidden\": b1_3, \n                \"W_output\": W2_3, \n                \"b_output\": b2_3, \n                \"eta\": .001, \n                \"hidden_activation\": activation_array[0], \n                \"hidden_activation_d\": activation_d_array[0], \n                \"output_activation\": activation_array[1],\n                \"output_activation_d\": activation_d_array[1], \n                \"loss_function\": cce,\n                \"loss_function_d\": cce_d\n                }\n\nmlp_object_train_3 = data.MLP(**kwargs_train_3)\n\nepoch_losses_3 = {100: [], 300: [], 500: []}\nepoch_accuracy_3 = {}\n\nbatch_size = 32\nN = train_sample_norm_3.shape[1]\n\nfor n_epochs, losses in epoch_losses_3.items():\n    epoch_accuracy_3[n_epochs] = []\n\n    for epoch in range(n_epochs):\n        epoch_correct = 0\n        epoch_count = 0\n        epoch_loss_accum = 0.0\n\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n\n            sample_batch = train_sample_norm_3[:, start:end]\n            labels_batch = train_labels_onehot_3[:, start:end]\n\n            mlp_object_train_3.input = sample_batch\n            mlp_object_train_3.output = labels_batch\n\n            z1_pre_train_3, z1_activation_train_3, z2_pre_train_3, z2_activation_train_3 = mlp_object_train_3.forward(\n            )\n\n            # ==============================================================\n            # Armazenando a loss para plotar no gr\u00e1fico depois\n            loss = mlp_object_train_3.loss_calculation(labels_batch,\n                                                    z2_activation_train_3)\n            if np.ndim(loss) &gt; 0:\n                loss = np.mean(loss)\n\n            B = end - start\n            epoch_loss_accum += loss * B\n            # ==============================================================\n\n            # ==============================================================\n            # Armazenando a acur\u00e1cia do modelo para plotar no gr\u00e1fico depois\n            preds_idx = np.argmax(z2_activation_train_3, axis=0)\n            true_idx = np.argmax(labels_batch, axis=0)\n            epoch_correct += np.sum(preds_idx == true_idx)\n            epoch_count += B\n            # ==============================================================\n\n            # ==============================================================\n            # Backpropagation\n            dW1_train_3, db1_train_3, dW2_train_3, db2_train_3 = mlp_object_train_3.backpropagation(\n                z1_pre_train_3, z1_activation_train_3, z2_pre_train_3,\n                z2_activation_train_3)\n            # ==============================================================\n\n            # ==============================================================\n            # Ajustando os par\u00e2metros para o pr\u00f3ximo batch\n            W_hidden_train_3, b_hidden_train_3, W_output_train_3, b_output_train_3 = mlp_object_train_3.update_weights(\n                dW1_train_3, db1_train_3, dW2_train_3, db2_train_3)\n            # ==============================================================\n\n        epoch_loss = epoch_loss_accum / epoch_count\n        epoch_acc = epoch_correct / epoch_count\n\n        losses.append(epoch_loss)\n        epoch_accuracy_3[n_epochs].append(epoch_acc)\n</code></pre> <p>Dessa vez, fazemos o treinamento em batches como tentativa de aumentar a qualidade do modelo. </p> <ul> <li>Passo 4</li> </ul> Teste main.py<pre><code>kwargs_test_3 = {\n                \"input\": test_sample_norm_3, \n                \"output\": test_sample_labels_3, \n                \"W_hidden\": W_hidden_train_3, \n                \"b_hidden\": b_hidden_train_3, \n                \"W_output\": W_output_train_3, \n                \"b_output\": b_output_train_3, \n                \"eta\": .001, \n                \"hidden_activation\": activation_array[0], \n                \"hidden_activation_d\": activation_d_array[0], \n                \"output_activation\": activation_array[1],\n                \"output_activation_d\": activation_d_array[1], \n                \"loss_function\": cce,\n                \"loss_function_d\": cce_d\n            }\n\nmlp_object_test_3 = data.MLP(**kwargs_test_3)\n\ndef accuracy_from_preds(z2_act, y_true):\n    # z2_act: (M, N), y_true: one-hot (M, N) ou indices (N,)\n    y_pred_idx = np.argmax(z2_act, axis=0)\n    if y_true.ndim == 2:\n        y_true_idx = np.argmax(y_true, axis=0)\n    else:\n        y_true_idx = y_true\n    return np.mean(y_pred_idx == y_true_idx), y_pred_idx, y_true_idx\n\nz1, h1, z2, y_pred_test = mlp_object_test_3.forward()\nacc_test, preds_idx, true_idx = accuracy_from_preds(y_pred_test, test_sample_labels_3) \n</code></pre> <p>A acur\u00e1cia do modelo na amostragem de teste foi de \\(83.58\\%\\).</p>"}]}